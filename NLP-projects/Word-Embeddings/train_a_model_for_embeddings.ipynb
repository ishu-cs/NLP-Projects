{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the word embeddings for a corpous of our own data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ishu_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "#This tokenizer divides a text into a list of sentences by using an unsupervised algorithm to build a model for abbreviation words, \n",
    "#collocations, and words that start sentences. It must be trained on a large collection of plaintext in the target language before it can be used.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim # Gensim is a Python library for topic modelling, document indexing and similarity retrieval with large corpora.\n",
    "from gensim.models import Word2Vec,KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaggle\n",
      "  Downloading kaggle-1.6.6.tar.gz (84 kB)\n",
      "     ---------------------------------------- 84.6/84.6 KB ? eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six>=1.10 in d:\\data-science\\gen_ai\\nlp\\nlp-projects\\nlp-projects\\lib\\site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: certifi in d:\\data-science\\gen_ai\\nlp\\nlp-projects\\nlp-projects\\lib\\site-packages (from kaggle) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil in d:\\data-science\\gen_ai\\nlp\\nlp-projects\\nlp-projects\\lib\\site-packages (from kaggle) (2.9.0.post0)\n",
      "Requirement already satisfied: requests in d:\\data-science\\gen_ai\\nlp\\nlp-projects\\nlp-projects\\lib\\site-packages (from kaggle) (2.31.0)\n",
      "Requirement already satisfied: tqdm in d:\\data-science\\gen_ai\\nlp\\nlp-projects\\nlp-projects\\lib\\site-packages (from kaggle) (4.66.2)\n",
      "Collecting python-slugify\n",
      "  Downloading python_slugify-8.0.4-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: urllib3 in d:\\data-science\\gen_ai\\nlp\\nlp-projects\\nlp-projects\\lib\\site-packages (from kaggle) (2.2.1)\n",
      "Collecting bleach\n",
      "  Using cached bleach-6.1.0-py3-none-any.whl (162 kB)\n",
      "Collecting webencodings\n",
      "  Using cached webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Collecting text-unidecode>=1.3\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.2/78.2 KB ? eta 0:00:00\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\data-science\\gen_ai\\nlp\\nlp-projects\\nlp-projects\\lib\\site-packages (from requests->kaggle) (3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\data-science\\gen_ai\\nlp\\nlp-projects\\nlp-projects\\lib\\site-packages (from requests->kaggle) (3.3.2)\n",
      "Requirement already satisfied: colorama in d:\\data-science\\gen_ai\\nlp\\nlp-projects\\nlp-projects\\lib\\site-packages (from tqdm->kaggle) (0.4.6)\n",
      "Using legacy 'setup.py install' for kaggle, since package 'wheel' is not installed.\n",
      "Installing collected packages: webencodings, text-unidecode, python-slugify, bleach, kaggle\n",
      "  Running setup.py install for kaggle: started\n",
      "  Running setup.py install for kaggle: finished with status 'done'\n",
      "Successfully installed bleach-6.1.0 kaggle-1.6.6 python-slugify-8.0.4 text-unidecode-1.3 webencodings-0.5.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'D:\\Data-Science\\Gen_AI\\NLP\\NLP-Projects\\NLP-projects\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "## install kaggle api\n",
    "! pip install kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the kaggle redit dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r'D:\\Data-Science\\Gen_AI\\NLP\\NLP-Projects\\NLP-projects\\Word-Embeddings\\reddit_worldnews_start_to_2016-11-22.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_created</th>\n",
       "      <th>date_created</th>\n",
       "      <th>up_votes</th>\n",
       "      <th>down_votes</th>\n",
       "      <th>title</th>\n",
       "      <th>over_18</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1201232046</td>\n",
       "      <td>2008-01-25</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Scores killed in Pakistan clashes</td>\n",
       "      <td>False</td>\n",
       "      <td>polar</td>\n",
       "      <td>worldnews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1201232075</td>\n",
       "      <td>2008-01-25</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Japan resumes refuelling mission</td>\n",
       "      <td>False</td>\n",
       "      <td>polar</td>\n",
       "      <td>worldnews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1201232523</td>\n",
       "      <td>2008-01-25</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>US presses Egypt on Gaza border</td>\n",
       "      <td>False</td>\n",
       "      <td>polar</td>\n",
       "      <td>worldnews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1201233290</td>\n",
       "      <td>2008-01-25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Jump-start economy: Give health care to all</td>\n",
       "      <td>False</td>\n",
       "      <td>fadi420</td>\n",
       "      <td>worldnews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1201274720</td>\n",
       "      <td>2008-01-25</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Council of Europe bashes EU&amp;UN terror blacklist</td>\n",
       "      <td>False</td>\n",
       "      <td>mhermans</td>\n",
       "      <td>worldnews</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   time_created date_created  up_votes  down_votes  \\\n",
       "0    1201232046   2008-01-25         3           0   \n",
       "1    1201232075   2008-01-25         2           0   \n",
       "2    1201232523   2008-01-25         3           0   \n",
       "3    1201233290   2008-01-25         1           0   \n",
       "4    1201274720   2008-01-25         4           0   \n",
       "\n",
       "                                             title  over_18    author  \\\n",
       "0                Scores killed in Pakistan clashes    False     polar   \n",
       "1                 Japan resumes refuelling mission    False     polar   \n",
       "2                  US presses Egypt on Gaza border    False     polar   \n",
       "3     Jump-start economy: Give health care to all     False   fadi420   \n",
       "4  Council of Europe bashes EU&UN terror blacklist    False  mhermans   \n",
       "\n",
       "   subreddit  \n",
       "0  worldnews  \n",
       "1  worldnews  \n",
       "2  worldnews  \n",
       "3  worldnews  \n",
       "4  worldnews  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(509236, 8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will perform NLP on title column of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_titles = df['title'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenise the word in each title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Scores', 'killed', 'in', 'Pakistan', 'clashes']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## test for first title\n",
    "nltk.word_tokenize('Scores killed in Pakistan clashes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vec = [nltk.word_tokenize(title) for title in news_titles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Scores', 'killed', 'in', 'Pakistan', 'clashes']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_vec[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(new_vec,min_count=1,vector_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x1ee41a7f8b0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.114623  ,  1.7692492 ,  1.5911435 ,  1.3579772 , -3.8580768 ,\n",
       "       -1.7055107 , -4.9927545 , -1.6688102 ,  2.4175835 , -2.035325  ,\n",
       "       -5.98303   , -2.007966  , -2.2277548 ,  3.5544894 , -1.5461595 ,\n",
       "       -2.4562962 , -1.7954701 ,  3.9541323 ,  0.836916  ,  1.3135964 ,\n",
       "       -0.96333945, -0.7016268 , -2.4511368 , -3.3794372 , -2.4683485 ,\n",
       "        2.1790175 , -1.0284032 , -4.1720247 , -1.4453021 ,  4.6200275 ,\n",
       "       -0.82480323,  1.1530538 ], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## See the vector\n",
    "model.wv['men']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Torture', 0.9055806398391724),\n",
       " ('Campaign', 0.8836544752120972),\n",
       " ('Threats', 0.8797277212142944),\n",
       " ('Propaganda', 0.8739300966262817),\n",
       " ('Groups', 0.8704061508178711),\n",
       " ('List', 0.8697134256362915),\n",
       " ('Espionage', 0.8693263530731201),\n",
       " ('Crime', 0.8632627129554749),\n",
       " ('Organized', 0.8552236557006836),\n",
       " ('Cell', 0.8519430160522461)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get the most similar words\n",
    "model.wv.most_similar('Terrorism')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Rignot', 0.6884018182754517),\n",
       " ('stone-crushing', 0.6622767448425293),\n",
       " ('unmonitored', 0.634545624256134),\n",
       " ('harmonise', 0.6322240233421326),\n",
       " ('ciesium', 0.6295283436775208),\n",
       " ('while', 0.6148161292076111),\n",
       " ('cocked', 0.6113882064819336),\n",
       " ('war-damaged', 0.5993403196334839),\n",
       " ('plant-', 0.5945883393287659),\n",
       " ('ladder-not', 0.5786762833595276)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(model.wv['pakistan']-model.wv['Terrorism'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Germany', 0.7507551312446594),\n",
       " ('Portugal', 0.7288891077041626),\n",
       " ('Hungary', 0.7229740619659424),\n",
       " ('Spain', 0.7156744599342346),\n",
       " ('Greece', 0.7133193612098694),\n",
       " ('Finland', 0.7129879593849182),\n",
       " ('Denmark', 0.696727454662323),\n",
       " ('Austria', 0.6945816278457642),\n",
       " ('Italy', 0.6908759474754333),\n",
       " ('Croatia', 0.6889337301254272)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(model.wv['Germany']-model.wv['Paris']+model.wv['Berlin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP-projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
